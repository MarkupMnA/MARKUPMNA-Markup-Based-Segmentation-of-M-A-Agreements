{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwrV512Wds1g"
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmVpIfmvds1j",
    "outputId": "b250c696-2bfc-4390-c6ef-329f4d091d4c"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import os \n",
    "import glob\n",
    "from transformers import MarkupLMProcessor\n",
    "from transformers import MarkupLMForTokenClassification\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "# import input_pipeline as ip\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "\n",
    "1. create_raw_dataset: takes the tagged csvs and creates a dict of \n",
    "    xpaths, nodes, node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_dataset(tagged_csv_path, id2label, label2id, is_train=True):\n",
    "    \"\"\"Preprocesses the tagged csvs in the format required by MarkupLM\"\"\"\n",
    "\n",
    "    tagged_df = pd.read_csv(tagged_csv_path)\n",
    "\n",
    "    # in train mode we expect text and xpaths that are highlighted \n",
    "    # by an annotator\n",
    "    if is_train:\n",
    "        col_list = [\"nodes\", \"xpaths\", \"node_labels\"]\n",
    "        \n",
    "        tagged_df[\"highlighted_xpaths\"] = tagged_df[\"highlighted_xpaths\"].fillna(\n",
    "            tagged_df[\"xpaths\"]\n",
    "        )\n",
    "        tagged_df[\"highlighted_segmented_text\"] = tagged_df[\n",
    "            \"highlighted_segmented_text\"\n",
    "        ].fillna(tagged_df[\"text\"])\n",
    "\n",
    "        # drop non-ASCII chars\n",
    "        tagged_df[\"highlighted_segmented_text\"] = (\n",
    "            tagged_df[\"highlighted_segmented_text\"]\n",
    "            .str.encode(\"ascii\", errors=\"ignore\")\n",
    "            .str.decode(\"ascii\")\n",
    "        )\n",
    "\n",
    "        # rename columns to match MarkupLM convention\n",
    "        tagged_df = tagged_df.rename(\n",
    "            columns={\n",
    "                \"highlighted_xpaths\": \"xpaths\",\n",
    "                \"highlighted_segmented_text\": \"nodes\",\n",
    "                \"tagged_sequence\": \"node_labels\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # convert node labels to integer values\n",
    "        tagged_df[\"node_labels\"] = tagged_df[\"node_labels\"].apply(\n",
    "            lambda label: label2id[label]\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        col_list = [\"nodes\", \"xpaths\"]\n",
    "        \n",
    "        # rename columns to match MarkupLM convention\n",
    "        tagged_df = tagged_df.rename(\n",
    "            columns={\n",
    "                \"xpaths\": \"xpaths\",\n",
    "                \"text\": \"nodes\",\n",
    "            },\n",
    "        )     \n",
    "    \n",
    "    tagged_output = tagged_df.loc[:, col_list].to_dict(orient=\"list\")\n",
    "\n",
    "    # convert each key to a list of lists just like the MarkupLM\n",
    "    # pipeline requires\n",
    "    for k, v in tagged_output.items():\n",
    "        tagged_output[k] = [v]\n",
    "\n",
    "    return tagged_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkupLMDataset(Dataset):\n",
    "    \"\"\"Dataset for token classification with MarkupLM.\"\"\"\n",
    "\n",
    "    def __init__(self, data, processor=None, max_length=512, is_train=True):\n",
    "        self.data = data\n",
    "        self.is_train = is_train\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.encodings = []\n",
    "        self.get_encoding_windows()\n",
    "        \n",
    "\n",
    "    def get_encoding_windows(self):\n",
    "        \"\"\"Splits the tokenized input into windows of 512 tokens\"\"\"\n",
    "                \n",
    "        for item in self.data:            \n",
    "            if self.is_train:\n",
    "                nodes, xpaths, node_labels = (\n",
    "                    item[\"nodes\"],\n",
    "                    item[\"xpaths\"],\n",
    "                    item['node_labels']\n",
    "                )\n",
    "            else:\n",
    "                nodes, xpaths, node_labels = (\n",
    "                    item[\"nodes\"],\n",
    "                    item[\"xpaths\"],\n",
    "                    None\n",
    "                )                \n",
    "            \n",
    "            # provide encoding to processor\n",
    "            encoding = self.processor(\n",
    "                nodes=nodes,\n",
    "                xpaths=xpaths,\n",
    "                node_labels=node_labels,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=False,\n",
    "                return_offsets_mapping=True\n",
    "            )\n",
    "\n",
    "            # remove batch dimension\n",
    "            encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "\n",
    "            # chunk up the encoding sequences to that it is less than the \n",
    "            # max input length of 512 tokens\n",
    "            if not self.is_train:\n",
    "                num_tokens = len(item['nodes'][0])\n",
    "                \n",
    "                for idx in range(0, num_tokens, self.max_length):\n",
    "                    batch_encoding = {}\n",
    "                    for k, v in encoding.items():\n",
    "                        batch_encoding[k] = v[idx: idx + self.max_length]\n",
    "\n",
    "                    self.encodings.append(batch_encoding)                    \n",
    "                    continue\n",
    "            \n",
    "            else:\n",
    "                if len(encoding[\"input_ids\"]) <= self.max_length:                    \n",
    "                    self.encodings.append(encoding)\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    batch_encoding = {}\n",
    "\n",
    "                    start_idx, end_idx = 0, self.max_length\n",
    "\n",
    "                    while end_idx < len(encoding[\"input_ids\"]):\n",
    "                        # decrement the end_idx by 1 until the label is not -100\n",
    "                        while encoding[\"labels\"][end_idx] == -100:\n",
    "                            end_idx = end_idx - 1\n",
    "\n",
    "                            # if the end idx is equal to the start idx meaning\n",
    "                            # we don't encounter a non -100 token,\n",
    "                            # we set window size as the max_length\n",
    "                            if end_idx == start_idx:\n",
    "                                end_idx = start_idx + self.max_length\n",
    "                                break\n",
    "\n",
    "                        for k, v in encoding.items():\n",
    "                            batch_encoding[k] = v[start_idx:end_idx]\n",
    "\n",
    "                        self.encodings.append(batch_encoding)\n",
    "                        batch_encoding = {}\n",
    "\n",
    "                        # update the pointers\n",
    "                        start_idx = end_idx\n",
    "                        end_idx = end_idx + self.max_length\n",
    "\n",
    "                    # collect the remaining tokens\n",
    "                    for k, v in encoding.items():\n",
    "                        batch_encoding[k] = v[start_idx:]\n",
    "\n",
    "                    if batch_encoding:\n",
    "                        self.encodings.append(batch_encoding)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # first, get nodes, xpaths and node labels\n",
    "        item = self.encodings[idx]\n",
    "\n",
    "        # pad the encodings to max_length of 512 tokens\n",
    "        padded_item = self.processor.tokenizer.pad(\n",
    "            item, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return padded_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inference Loop and Main Function Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_loop(dataloader, model, device, config, processor):\n",
    "    '''Runs eval loop for entire dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader: torch.utils.data.DataLoader: iterator over Dataset object\n",
    "        model: transformers.PreTrainedModel. fine-tuned MarkupLM model\n",
    "        device: torch.device. Specifies whether GPU is available for computation\n",
    "        label_list: list. List of labels used to train the MarkupLM model\n",
    "        config: dict. Contains user-provided params and args\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    results = {\"nodes\": [], \"preds\": []}\n",
    "    for batch in tqdm(dataloader, desc='inference_loop'):\n",
    "        # get the inputs;\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # if ablation mode is set to true then\n",
    "        # either mask the xpaths or shuffle them\n",
    "        if config[\"ablation\"][\"run_ablation\"]:\n",
    "            inputs = utils.ablation(config, inputs)\n",
    "\n",
    "        # get the offset mapping. It contains the spans of the \n",
    "        # words that were split during tokenization. \n",
    "        # Info present at a token level\n",
    "        offset_mapping = inputs.pop('offset_mapping').squeeze().tolist()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        pred_labels = [model.config.id2label[id] for id in predictions.squeeze().tolist()]\n",
    "        \n",
    "        input_ids = inputs['input_ids'].detach().numpy().flatten().tolist()\n",
    "        input_word_pieces = [processor.decode([id]) for id in input_ids]\n",
    "        \n",
    "        \n",
    "        # input_ids = [x for x in input_ids if x not in special_tokens]\n",
    "        # print(input_ids)\n",
    "        results['nodes'].append(input_word_pieces)\n",
    "        results['preds'].append(pred_labels)\n",
    "                                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config, test_data, model_ckpt_path=None, is_train=False):\n",
    "    '''Main execution of script'''\n",
    "    # get the  list of labels along with the label to id mapping and\n",
    "    # reverse mapping\n",
    "    label_list, id2label, label2id = utils.get_label_list(config)\n",
    "    \n",
    "    # define the processor and model\n",
    "    if config[\"model\"][\"use_large_model\"]:\n",
    "        processor = MarkupLMProcessor.from_pretrained(\n",
    "            \"microsoft/markuplm-large\",\n",
    "            only_label_first_subword=config['model']['label_only_first_subword']\n",
    "        )\n",
    "        model = MarkupLMForTokenClassification.from_pretrained(\n",
    "            \"microsoft/markuplm-large\", id2label=id2label, label2id=label2id\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        processor = MarkupLMProcessor.from_pretrained(\n",
    "            \"microsoft/markuplm-base\",\n",
    "            only_label_first_subword=config['model']['label_only_first_subword'],\n",
    "        )\n",
    "        model = MarkupLMForTokenClassification.from_pretrained(\n",
    "            \"microsoft/markuplm-base\", id2label=id2label, label2id=label2id\n",
    "        )\n",
    "        \n",
    "    if model_ckpt_path is not None:\n",
    "        model_ckpt = torch.load(model_ckpt_path, \n",
    "                                map_location='cpu')\n",
    "        \n",
    "        model.load_state_dict(model_ckpt)\n",
    "        \n",
    "\n",
    "    processor.parse_html = False\n",
    "    \n",
    "    # convert the input dataset\n",
    "    # to torch datasets. Create the dataloaders as well\n",
    "    test_dataset = MarkupLMDataset(\n",
    "        data=test_data,\n",
    "        processor=processor,\n",
    "        max_length=config[\"model\"][\"max_length\"],\n",
    "        is_train=is_train\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model.to(device)  # move to GPU if available\n",
    "\n",
    "    print(\"*\" * 50)\n",
    "    print(f'Running Inference Loop!')\n",
    "    print(\"*\" * 50)\n",
    "\n",
    "    # run inference loop\n",
    "    results = run_inference_loop(test_dataloader, model, device, \n",
    "                                     config, processor)\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the input params\n",
    "\n",
    "Trained model checkpoints are available in this Google Drive directory: https://drive.google.com/drive/folders/1SGeSA9OLBYcpl_l0SFIjaP445KfEzcp-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './configs/config.yaml'\n",
    "\n",
    "test_contract_dir = \"../contracts/test\"\n",
    "\n",
    "# if loading from ckpt then change the line below\n",
    "model_ckpt_path = \"/Users/sukritrao/Documents/NYU/Coursework/Spring2023/Independent-Study/project/results/markuplm_model_run2_num_contract-30pct_epoch-13_f1-0.871.pt\"\n",
    "\n",
    "max_length = 512\n",
    "test_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the config file \n",
    "with open(config_path, 'r') as yml:\n",
    "    config = yaml.safe_load(yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list, id2label, label2id = utils.get_label_list(config)\n",
    "num_labels = len(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 in test dir\n"
     ]
    }
   ],
   "source": [
    "test_contracts = glob.glob(os.path.join(test_contract_dir, \"*.csv\"))\n",
    "\n",
    "test_contracts = [test_contracts[0]]\n",
    "\n",
    "print(f\"Found {len(test_contracts)} in test dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the input csvs obtained from webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../contracts/test/contract_69.csv'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_contracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xpaths</th>\n",
       "      <th>text</th>\n",
       "      <th>highlighted_xpaths</th>\n",
       "      <th>highlighted_segmented_text</th>\n",
       "      <th>tagged_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/html/body/document/type/sequence</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/html/body/document/type/sequence/filename</td>\n",
       "      <td>d222750dex21.htm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              xpaths              text  \\\n",
       "0                           /html/body/document/type            EX-2.1   \n",
       "1                  /html/body/document/type/sequence                 2   \n",
       "2         /html/body/document/type/sequence/filename  d222750dex21.htm   \n",
       "3  /html/body/document/type/sequence/filename/des...            EX-2.1   \n",
       "4  /html/body/document/type/sequence/filename/des...            EX-2.1   \n",
       "\n",
       "  highlighted_xpaths highlighted_segmented_text tagged_sequence  \n",
       "0                NaN                        NaN               o  \n",
       "1                NaN                        NaN               o  \n",
       "2                NaN                        NaN               o  \n",
       "3                NaN                        NaN               o  \n",
       "4                NaN                        NaN               o  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = pd.read_csv(test_contracts[0])\n",
    "\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [] \n",
    "for tagged_path in test_contracts:\n",
    "    tagged_output = create_raw_dataset(tagged_path, \n",
    "                                       id2label=id2label, \n",
    "                                       label2id=label2id,\n",
    "                                       is_train=False)\n",
    "\n",
    "    test_data.append(tagged_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in test_data[0].items():\n",
    "  print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['markuplm.pooler.dense.bias', 'cls.predictions.bias', 'nrp_cls.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.transform.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.bias', 'nrp_cls.decoder.weight', 'cls.predictions.decoder.weight', 'ptc_cls.weight', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'nrp_cls.decoder.bias', 'markuplm.pooler.dense.weight', 'ptc_cls.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e850ad17e4c428db0972f4009909319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "inference_loop:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "results = main(config, test_data, model_ckpt_path=model_ckpt_path,is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode(['nodes', 'preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EX</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>s_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nodes preds\n",
       "0   <s>     o\n",
       "0    EX     o\n",
       "0     -   s_n\n",
       "0     2     o\n",
       "0     .     o"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09500e44e8424bda9c121da49ebeb1df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ee906c2608a4712bc63eda81b172809": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19749a6a48cc4a1d91abff0fe43df1b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4fde29a12a8743d99293170cf49671ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eefd6b2dbaec4c978ce6d5b81bb91d0c",
      "max": 225,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_19749a6a48cc4a1d91abff0fe43df1b6",
      "value": 0
     }
    },
    "517ebaa35e2d42ba9837c35fcd6eee84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ee906c2608a4712bc63eda81b172809",
      "placeholder": "​",
      "style": "IPY_MODEL_09500e44e8424bda9c121da49ebeb1df",
      "value": "  0%"
     }
    },
    "56a6dc9eac5b41a494e636bb685be1ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "632672424cc04daf8a5ee6b5d7b23809": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93c6897eb85e4240b05f446999e21353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_517ebaa35e2d42ba9837c35fcd6eee84",
       "IPY_MODEL_4fde29a12a8743d99293170cf49671ba",
       "IPY_MODEL_d15852a8683c4058b77c0c571a426f35"
      ],
      "layout": "IPY_MODEL_56a6dc9eac5b41a494e636bb685be1ae"
     }
    },
    "bfdd1b97a07c46919ff0fb6f1b580e47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d15852a8683c4058b77c0c571a426f35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_632672424cc04daf8a5ee6b5d7b23809",
      "placeholder": "​",
      "style": "IPY_MODEL_bfdd1b97a07c46919ff0fb6f1b580e47",
      "value": " 0/225 [00:00&lt;?, ?it/s]"
     }
    },
    "eefd6b2dbaec4c978ce6d5b81bb91d0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
